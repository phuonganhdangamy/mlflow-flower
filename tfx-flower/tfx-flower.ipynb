{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41adadce",
   "metadata": {},
   "source": [
    "# TFX  pipeline in Flower framework\n",
    "\n",
    "Second attempt\n",
    "\n",
    "The notebook is used for local testing, replicating Flower's tutorial to implement client and server at the same place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdb28cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\phuon\\miniconda3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-06-20 21:57:02,020\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on cpu\n",
      "Flower 1.18.0 / PyTorch 2.2.2+cpu\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import glob\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "\n",
    "import flwr\n",
    "from flwr.client import Client, ClientApp, NumPyClient\n",
    "from flwr.server import ServerApp, ServerConfig, ServerAppComponents\n",
    "from flwr.server.strategy import FedAvg, FedAdagrad\n",
    "from flwr.simulation import run_simulation, start_simulation\n",
    "from flwr_datasets import FederatedDataset\n",
    "from flwr.common import ndarrays_to_parameters, NDArrays, Scalar, Context\n",
    "\n",
    "DEVICE = torch.device(\"cpu\")  # Try \"cuda\" to train on GPU\n",
    "print(f\"Training on {DEVICE}\")\n",
    "print(f\"Flower {flwr.__version__} / PyTorch {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcb4856",
   "metadata": {},
   "outputs": [],
   "source": [
    "import absl\n",
    "import tensorflow_model_analysis as tfma\n",
    "from tfx.components import CsvExampleGen\n",
    "from tfx.components import Evaluator\n",
    "from tfx.components import ExampleValidator\n",
    "from tfx.components import Pusher\n",
    "from tfx.components import SchemaGen\n",
    "from tfx.components import StatisticsGen\n",
    "from tfx.components import Trainer\n",
    "from tfx.components import Transform\n",
    "from tfx.dsl.components.common import resolver\n",
    "from tfx.dsl.experimental import latest_blessed_model_resolver\n",
    "from tfx.orchestration import metadata\n",
    "from tfx.orchestration import pipeline\n",
    "from tfx.orchestration.beam.beam_dag_runner import BeamDagRunner\n",
    "from tfx.proto import pusher_pb2\n",
    "from tfx.proto import trainer_pb2\n",
    "from tfx.types import Channel\n",
    "from tfx.types.standard_artifacts import Model\n",
    "from tfx.types.standard_artifacts import ModelBlessing\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d6dbc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipeline(client_id: int, module_file: str, output_dir: str) -> pipeline.Pipeline:\n",
    "    data_root = f\"../tfx-flower/data/simple/client_{client_id}.csv\"\n",
    "    pipeline_root = f\"{output_dir}/pipeline_client_{client_id}\"\n",
    "    # metadata_path = f\"{output_dir}/metadata_client_{client_id}.db\"\n",
    "    serving_model_dir = os.path.join(output_dir, f\"serving_model_client_{client_id}\")\n",
    "\n",
    "    # Brings data into the pipeline or otherwise joins/converts training data.\n",
    "    example_gen = CsvExampleGen(input_base=os.path.dirname(data_root))\n",
    "    \n",
    "    # Computes statistics over data for visualization and example validation.\n",
    "    statistics_gen = StatisticsGen(examples=example_gen.outputs['examples'])\n",
    "\n",
    "    # Generates schema based on statistics files.\n",
    "    schema_gen = SchemaGen(\n",
    "        statistics=statistics_gen.outputs['statistics'],\n",
    "        infer_feature_shape=True)\n",
    "    \n",
    "    # Performs anomaly detection based on statistics and data schema.\n",
    "    example_validator = ExampleValidator(\n",
    "        statistics=statistics_gen.outputs['statistics'],\n",
    "        schema=schema_gen.outputs['schema'])\n",
    "        \n",
    "    # Performs transformations and feature engineering in training and serving.\n",
    "    transform = Transform(\n",
    "        examples=example_gen.outputs['examples'],\n",
    "        schema=schema_gen.outputs['schema'],\n",
    "        module_file=module_file)\n",
    "    \n",
    "    return pipeline.Pipeline(\n",
    "        pipeline_name=f\"client_{client_id}_pipeline\",\n",
    "        pipeline_root=pipeline_root,\n",
    "        components=[example_gen, statistics_gen, schema_gen, example_validator, transform],\n",
    "        enable_cache=True,\n",
    "        metadata_connection_config=None,\n",
    "    )\n",
    "\n",
    "def run_pipeline(client_id: int, module_file: str, output_dir: str):\n",
    "    print(f\"Running TFX pipeline for client {client_id}...\")\n",
    "    BeamDagRunner().run(create_pipeline(client_id=client_id, module_file=module_file, output_dir=output_dir))\n",
    "    print(f\"Pipeline completed for client {client_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982b0609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run pipelines for all 5 clients\n",
    "MODULE_FILE = \"taxi_utils.py\"  # Your preprocessing module file\n",
    "OUTPUT_DIR = \"./tfx_output\"\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Run pipelines for all clients\n",
    "for client_id in range(5):\n",
    "    try:\n",
    "        run_pipeline(client_id, MODULE_FILE, OUTPUT_DIR)\n",
    "    except Exception as e:\n",
    "        print(f\"Error running pipeline for client {client_id}: {e}\")\n",
    "\n",
    "print(\"All TFX pipelines completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcaf321",
   "metadata": {},
   "source": [
    "## Define the model\n",
    "DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa1170b",
   "metadata": {},
   "source": [
    "We define constants from the original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481e93e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical features are assumed to each have a maximum value in the dataset.\n",
    "_MAX_CATEGORICAL_FEATURE_VALUES = [24, 31, 13, 2000, 2000, 80, 80]\n",
    "\n",
    "_CATEGORICAL_FEATURE_KEYS = [\n",
    "    'trip_start_hour', 'trip_start_day', 'trip_start_month',\n",
    "    'pickup_census_tract', 'dropoff_census_tract', 'pickup_community_area',\n",
    "    'dropoff_community_area'\n",
    "]\n",
    "\n",
    "_DENSE_FLOAT_FEATURE_KEYS = ['trip_miles', 'fare', 'trip_seconds']\n",
    "\n",
    "# Number of buckets used by tf.transform for encoding each feature.\n",
    "_FEATURE_BUCKET_COUNT = 10\n",
    "\n",
    "_BUCKET_FEATURE_KEYS = [\n",
    "    'pickup_latitude', 'pickup_longitude', 'dropoff_latitude',\n",
    "    'dropoff_longitude'\n",
    "]\n",
    "\n",
    "# Number of vocabulary terms used for encoding VOCAB_FEATURES by tf.transform\n",
    "_VOCAB_SIZE = 1000\n",
    "\n",
    "# Count of out-of-vocab buckets in which unrecognized VOCAB_FEATURES are hashed.\n",
    "_OOV_SIZE = 10\n",
    "\n",
    "_VOCAB_FEATURE_KEYS = [\n",
    "    'payment_type',\n",
    "    'company',\n",
    "]\n",
    "\n",
    "# Keys\n",
    "_LABEL_KEY = 'tips'\n",
    "_FARE_KEY = 'fare'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b318632c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfx.components.trainer.fn_args_utils import DataAccessor\n",
    "from tfx_bsl.tfxio import dataset_options\n",
    "\n",
    "def _transformed_name(key):\n",
    "  return key + '_xf'\n",
    "\n",
    "def _transformed_names(keys):\n",
    "  return [_transformed_name(key) for key in keys]\n",
    "\n",
    "def _input_fn(file_pattern: List[str],\n",
    "              data_accessor: DataAccessor,\n",
    "              tf_transform_output: tft.TFTransformOutput,\n",
    "              batch_size: int = 200) -> tf.data.Dataset:\n",
    "  \"\"\"Generates features and label for tuning/training.\n",
    "\n",
    "  Args:\n",
    "    file_pattern: List of paths or patterns of input tfrecord files.\n",
    "    data_accessor: DataAccessor for converting input to RecordBatch.\n",
    "    tf_transform_output: A TFTransformOutput.\n",
    "    batch_size: representing the number of consecutive elements of returned\n",
    "      dataset to combine in a single batch\n",
    "\n",
    "  Returns:\n",
    "    A dataset that contains (features, indices) tuple where features is a\n",
    "      dictionary of Tensors, and indices is a single Tensor of label indices.\n",
    "  \"\"\"\n",
    "  return data_accessor.tf_dataset_factory(\n",
    "      file_pattern,\n",
    "      dataset_options.TensorFlowDatasetOptions(\n",
    "          batch_size=batch_size, label_key=_transformed_name(_LABEL_KEY)),\n",
    "      tf_transform_output.transformed_metadata.schema).repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21561f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(hidden_units: List[int]=None) -> tf.keras.Model:\n",
    "    \"\"\"Creates a DNN Keras model for classifying taxi data.\n",
    "    Mostly identical to the _build_keras_model in the taxi_utils_native_keras.py\n",
    "\n",
    "    Args:\n",
    "        hidden_units: [int], the layer sizes of the DNN (input layer first).\n",
    "\n",
    "    Returns:\n",
    "        A Wide and Deep keras Model.\n",
    "    \"\"\"\n",
    "    # Keras needs the feature definitions at compile time.\n",
    "    deep_input = {\n",
    "        colname: tf.keras.layers.Input(name=colname, shape=(1,), dtype=tf.float32)\n",
    "        for colname in _transformed_names(_DENSE_FLOAT_FEATURE_KEYS)\n",
    "    }\n",
    "\n",
    "    wide_vocab_input = {\n",
    "        colname: tf.keras.layer.Input(name=colname, shape=(1,), dtype='int32')\n",
    "        for colname in _transformed_names(_VOCAB_FEATURE_KEYS)\n",
    "    }\n",
    "\n",
    "    wide_bucket_input = {\n",
    "        colname: tf.keras.layers.Input(name=colname, shape=(1,), dttpe='int32')\n",
    "        for colname in _transformed_names(_BUCKET_FEATURE_KEYS)\n",
    "    }\n",
    "\n",
    "    wide_categorical_inpt = {\n",
    "        colname: tf.keras.layers.Input(name=colname, shape=(1,), dtype='int32')\n",
    "        for colname in _transformed_names(_CATEGORICAL_FEATURE_KEYS)\n",
    "    }\n",
    "\n",
    "    input_layers = {\n",
    "        **deep_input,\n",
    "        **wide_vocab_input,\n",
    "        **wide_bucket_input,\n",
    "        **wide_categorical_inpt,\n",
    "    }\n",
    "\n",
    "    # Build deep branch\n",
    "    deep = tf.keras.layers.concatenate(\n",
    "        [tf.keras.layers.Normalization()(layer) for layer in deep_input.values()]\n",
    "    )\n",
    "\n",
    "    for numnodes in (hidden_units or [100, 70, 50, 25]):\n",
    "        deep = tf.keras.layers.Dense(numnodes)(deep)\n",
    "\n",
    "    # Build wide branch\n",
    "    wide_layers = []\n",
    "\n",
    "    for key in _transformed_names(_VOCAB_FEATURE_KEYS):\n",
    "        wide_layers.append(\n",
    "            tf.keras.layers.CategoryEncoding(num_tokens=_VOCAB_SIZE + _OOV_SIZE)(\n",
    "                input_layers[key]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    for key in _transformed_names(_BUCKET_FEATURE_KEYS):\n",
    "        wide_layers.append(\n",
    "            tf.keras.layers.CategoryEncoding(num_tokens=_FEATURE_BUCKET_COUNT)(\n",
    "                input_layers[key]\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    for key, num_tokens in zip(\n",
    "        _transformed_names(_CATEGORICAL_FEATURE_KEYS),\n",
    "        _MAX_CATEGORICAL_FEATURE_VALUES,\n",
    "    ):\n",
    "        wide_layers.append(\n",
    "            tf.keras.layers.CategoryEncoding(num_tokens=num_tokens + 1)(\n",
    "                input_layers[key]\n",
    "            )\n",
    "        )\n",
    "\n",
    "    wide = tf.keras.layers.concatenate(wide_layers)\n",
    "\n",
    "    # Combine and create output\n",
    "    combined = tf.keras.layers.concatenate([deep, wide])\n",
    "    output = tf.keras.layers.Dense(1, activation='sigmoid')(combined)\n",
    "    output = tf.keras.layers.Reshape((1,))(output)\n",
    "\n",
    "    model = tf.keras.Model(inputs=input_layers, outputs=output)\n",
    "\n",
    "    # Compile\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        metrics=[tf.keras.metrics.BinaryAccuracy()],\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6dff70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model creating\n",
    "print(\"Testing model creation\")\n",
    "test_model = create_model()\n",
    "print(f\"Model created. Total parameters: {test_model.count_params()}\")\n",
    "test_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de073b4",
   "metadata": {},
   "source": [
    "## Data loading functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff58877",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tft \n",
    "\n",
    "def load_transformed_data(client_id: int, split: str='train', batch_size:int=32):\n",
    "    \"\"\"\n",
    "    Load preprocessed data from TFX Transform component output\n",
    "\n",
    "    Args:\n",
    "        client_id:\n",
    "        split: data split ('train' or 'eval')\n",
    "        batch_size: batch size for dataset\n",
    "\n",
    "    Returns:\n",
    "        tf.data.Dataset with preprocessed features and labels\n",
    "    \"\"\"\n",
    "    # Paths to transformed data\n",
    "    transform_output_path = f\"{OUTPUT_DIR}/pipeline_client_{client_id}/Transform/transform_graph\"\n",
    "    data_path = f\"{OUTPUT_DIR}/pipeline_client_{client_id}/Transform/transformed_examples/{split}/*.gz\"\n",
    "\n",
    "    try:\n",
    "        # Load the transformed output\n",
    "        tf_transform_output = tft.TransformOutput(transform_output_path)\n",
    "\n",
    "        # Find tfrecord files\n",
    "        tfrecord_files = glob.glob(data_path)\n",
    "        if not tfrecord_files:\n",
    "            raise FileNotFoundError(f\"No tfrecord files found at {data_path}\")\n",
    "        \n",
    "        print(f\"Loading data for client {client_id}, split: {split}\")\n",
    "        print(f\"Found {len(tfrecord_files)} tfrecord files\")\n",
    "\n",
    "        # Create dataset from tfrecord files\n",
    "        dataset = tf.data.TFRecordDataset(tfrecord_files, compression_type='GZIP')\n",
    "\n",
    "        # Parse the examples using the transformed schema\n",
    "        feature_spec = tf_transform_output.transformed_feature_spec()\n",
    "\n",
    "        def parse_fn(example_proto):\n",
    "            parsed = tf.io.parse_single_example(example_proto, feature_spec)\n",
    "\n",
    "            label_key = _transformed_name(_LABEL_KEY)\n",
    "            label = parsed.pop(label_key)\n",
    "\n",
    "            return parsed, label\n",
    "        \n",
    "        dataset = dataset.map(parse_fn, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "        dataset = dataset.batch(batch_size)\n",
    "        dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "        # check dataset\n",
    "        for batch in dataset.take(1):\n",
    "            features, labels = batch\n",
    "            print(f\"Feature keys: {list(features.keys())}\")\n",
    "            print(f\"Batch shape - Features: {len(features)}, Labels: {labels.shape}\")\n",
    "            break\n",
    "\n",
    "        return dataset \n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data for client {client_id}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d593f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data loading for client 0\n",
    "print(\"Test data loading\")\n",
    "test_dataset = load_transformed_data(0, 'train')\n",
    "if test_dataset:\n",
    "    print(\"Data loading successful\")\n",
    "else:\n",
    "    print(\"Data loading failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9624f122",
   "metadata": {},
   "source": [
    "\n",
    "## Flower Client Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6dab328",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TaxiFlowerClient(fl.client.NumPyClient):\n",
    "\n",
    "    def __init__(self, client_id: int):\n",
    "        self.client_id = client_id\n",
    "        self.model = create_model()\n",
    "\n",
    "        # Load client's data\n",
    "        self.train_dataset = load_transformed_data(client_id, 'train', batch_size=32)\n",
    "        self.eval_dataset = load_transformed_data(client_id, 'eval', batch_size=32)\n",
    "\n",
    "        self.train_size = sum(1 for _ in self.train_dataset.unbatch())\n",
    "        self.eval_size = sum(1 for _ in self.eval_dataset.unbatch())\n",
    "\n",
    "        print(f\"Client {client_id} initialized with {self.train_size} train examples and {self.eval_size} eval examples\")\n",
    "\n",
    "    def get_parameters(self, config):\n",
    "        return self.model.get_weights()\n",
    "    \n",
    "    def set_parameters(self, parameters) -> None:\n",
    "        return self.model.set_weights(parameters)\n",
    "    \n",
    "    def fit(self, parameters, config):\n",
    "        \"\"\"Train themodel on the local dataset\"\"\"\n",
    "        self.set_parameters(parameters)  # set parameters received from the server\n",
    "\n",
    "        epochs = int(config.get(\"epochs\", 1))\n",
    "\n",
    "        history = self.model.fit(\n",
    "            self.train_dataset,\n",
    "            epochs=epochs,\n",
    "            validation_data=self.eval_dataset,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        return (\n",
    "            self.get_parameters({}),\n",
    "            self.train_size,\n",
    "            {\n",
    "                \"train_loss\": float(history.history[\"loss\"][-1]),\n",
    "                \"train_accuracy\": float(history.history[\"binary_accuracy\"][-1])\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    def evaluate(self, parameters, config):\n",
    "        self.set_parameters(parameters)\n",
    "\n",
    "        loss, accuracy = self.model.evaluate(self.eval_dataset, verbose=0)\n",
    "\n",
    "        return float(loss), self.eval_size, {\"accuracy\": float(accuracy)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5611fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test client creation\n",
    "print(\"Test client creation\")\n",
    "try:\n",
    "    test_client = TaxiFlowerClient(0)\n",
    "    print(\"Client creation successful\")\n",
    "except Exception as e:\n",
    "    print(f\"Client creation failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d74fa23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def client_fn(cid: str) ->  TaxiFlowerClient:\n",
    "    client_id = int(cid)\n",
    "    return TaxiFlowerClient(client_id).to_client()\n",
    "\n",
    "# Create the ClientApp (for simulation)\n",
    "client = ClientApp(client_fn=client_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82475167",
   "metadata": {},
   "source": [
    "\n",
    "## Flower Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ef8a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_average(metrics: List[Tuple[int, Dict[str, float]]]) -> Dict[str, float]:\n",
    "    if not metrics:\n",
    "        return {}\n",
    "    \n",
    "    accuracies = [num_examples * m.get(\"accuracy\", 0.0) for num_examples, m in metrics]\n",
    "    examples = [num_examples for num_examples, _ in metrics]\n",
    "\n",
    "    total_examples = sum(examples)\n",
    "    if total_examples == 0:\n",
    "        return {\"accuracy\": 0.0}\n",
    "    \n",
    "    weighted_accuracy = sum(accuracies) / total_examples\n",
    "    return {\"accuracy\": weighted_accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286f627e",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLIENTS = 5\n",
    "NUM_ROUNDS = 5\n",
    "\n",
    "strategy = FedAvg(\n",
    "    fraction_fit=1.0,\n",
    "    fraction_evaluate=1.0,\n",
    "    min_fit_clients=NUM_CLIENTS,\n",
    "    min_evaluate_clients=NUM_CLIENTS,\n",
    "    min_available_clients=NUM_CLIENTS,\n",
    "    evaluate_metrics_aggregation_fn=weighted_average,\n",
    "    fit_metrics_aggregation_fn=weighted_average,\n",
    ")\n",
    "\n",
    "def server_fn(context: Context) -> ServerAppComponents:\n",
    "    \"\"\"Construct components that set the ServerApp behaviour.\n",
    "\n",
    "    You can use the settings in `context.run_config` to parameterize the\n",
    "    construction of all elements (e.g the strategy or the number of rounds)\n",
    "    wrapped in the returned ServerAppComponents object.\n",
    "    \"\"\"\n",
    "\n",
    "    # Configure the server for 3 rounds of training\n",
    "    config = ServerConfig(num_rounds=NUM_ROUNDS)\n",
    "\n",
    "    return ServerAppComponents(strategy=strategy, config=config)\n",
    "\n",
    "# Create the ServerApp\n",
    "server = ServerApp(server_fn=server_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcfc729",
   "metadata": {},
   "source": [
    "## Federated Learning Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abeed07",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U \"flwr[simulation]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb4645c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the resources each of your clients need\n",
    "# By default, each client will be allocated 1x CPU and 0x GPUs\n",
    "backend_config = {\"client_resources\": {\"num_cpus\": 1, \"num_gpus\": 0.0}}\n",
    "\n",
    "# When running on GPU, assign an entire GPU for each client\n",
    "if DEVICE == \"cuda\":\n",
    "    backend_config = {\"client_resources\": {\"num_cpus\": 1, \"num_gpus\": 1.0}}\n",
    "    # Refer to our Flower framework documentation for more details about Flower simulations\n",
    "    # and how to set up the `backend_config`\n",
    "\n",
    "# Global variable to store history for analysis\n",
    "fl_history = None\n",
    "\n",
    "# Run simulation\n",
    "try: \n",
    "    fl_history = run_simulation(\n",
    "        server_app=server,\n",
    "        client_app=client,\n",
    "        num_supernodes=NUM_CLIENTS,\n",
    "        backend_config=backend_config,\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"Error during federated learning: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e027e241",
   "metadata": {},
   "source": [
    "## Result analyis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284bc034",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a145ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_fl_results(history):\n",
    "    \"\"\"Analyze and visualize FL results\"\"\"\n",
    "    if history is None:\n",
    "        print(\"No history avaiable\")\n",
    "        return\n",
    "    \n",
    "    print(f\"History object type: {type(history)}\")\n",
    "    print(f\"Available attributes: {dir(history)}\")\n",
    "\n",
    "    # Containers for metrics\n",
    "    rounds_data = []\n",
    "\n",
    "    if hasattr(history, 'metrics_distributed'):\n",
    "        print(\"Found metrics_distributed\")\n",
    "        for round_num, metrics in history.metrics_distributed.items():\n",
    "            round_data = {\n",
    "                'round': round_num,\n",
    "                'distributed_accuracy': metrics.get('accuracy', None),\n",
    "            }\n",
    "            rounds_data.append(round_data)\n",
    "\n",
    "    if hasattr(history, 'metrics_centralized') and history.metrics_centralized:\n",
    "        print(\"Found metrics_centralized\")\n",
    "        for round_num, metrics in history.metrics_centralized.items():\n",
    "            round_data = next((r for r in rounds_data if r['round'] == round_num), None)\n",
    "            if round_data is None:\n",
    "                round_data = {'round': round_num}\n",
    "                rounds_data.append(round_data)\n",
    "            round_data['centralized_accuracy'] = metrics.get('accuracy', None)\n",
    "\n",
    "    if hasattr(history, 'losses_distributed') and history.losses_distributed:\n",
    "        print(\"Found losses_distributed\")\n",
    "        for round_num, loss in history.losses_distributed.items():\n",
    "            round_data = next((r for r in rounds_data if r['round'] == round_num), None)\n",
    "            if round_data is None:\n",
    "                round_data = {'round': round_num}\n",
    "                rounds_data.append(round_data)\n",
    "            round_data['distributed_loss'] = loss\n",
    "\n",
    "    if hasattr(history, 'losses_centralized') and history.losses_centralized:\n",
    "        print(\"Found losses_centralized\")\n",
    "        for round_num, loss in history.losses_centralized.items():\n",
    "            round_data = next((r for r in rounds_data if r['round'] == round_num), None)\n",
    "            if round_data is None:\n",
    "                round_data = {'round': round_num}\n",
    "                rounds_data.append(round_data)\n",
    "            round_data['centralized_loss'] = loss\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    if rounds_data:\n",
    "        df = pd.DataFrame(rounds_data).sort_values('round')\n",
    "        print(f\"\\nResults DataFrame:\")\n",
    "        print(df)\n",
    "\n",
    "        #plots\n",
    "        fig, axes = plt.subplits(2, 2, figsize=(15, 10))\n",
    "\n",
    "        # 1. Distributed accuracy\n",
    "        if 'distributed_accuracy' in df.columns and df['distributed_accuracy'].notna().any():\n",
    "            axes[0, 0].plot(df['round'], df['distributed_accuracy'], 'b-o', linewidth=2, markersize=6)\n",
    "            axes[0, 0].set_title('Distributed accuracy over rounds')\n",
    "            axes[0, 0].set_xlabel('Round')\n",
    "            axes[0, 0].set_ylabel('Accuracy')\n",
    "            axes[0, 0].grid(True, alpha=0.3)\n",
    "            axes[0, 0].set_ylim([0, 1])\n",
    "\n",
    "        # 2. Distributed loss\n",
    "        if 'distributed_loss' in df.columns and df['distributed_loss'].notna().any():\n",
    "            axes[0, 1].plot(df['round'], df['distributed_loss'], 'r-o', linewidth=2, markersize=6)\n",
    "            axes[0, 1].set_title('Distributed loss over rounds')\n",
    "            axes[0, 1].set_xlabel('Round')\n",
    "            axes[0, 1].set_ylabel('Loss')\n",
    "            axes[0, 1].gid(True, alpha=0.3)\n",
    "\n",
    "        # 3. Centralized vs Distributed accuracy\n",
    "        if ('centralized_accuracy' in df.columns and df['centralized_accuracy'].notna().any() and \n",
    "            'distributed_accuracy' in df.columns and df['distributed_accuracy'].notna().any()):\n",
    "            axes[1, 0].plot(df['round'], df['distributed_accuracy'], 'b-o', label='Distributed', linewidth=2)\n",
    "            axes[1, 0].plot(df['round'], df['centralized_accuracy'], 'g-s', label='Centralized', linewidth=2)\n",
    "            axes[1, 0].set_title('Accuracy comparison')\n",
    "            axes[1, 0].set_xlabel('Round')\n",
    "            axes[1, 0].set_ylabel('Accuracy')\n",
    "            axes[1, 0].legend()\n",
    "            axes[1, 0].grid(True, alpha=0.3)\n",
    "            axes[1, 0].set_ylim([0, 1])\n",
    "        elif 'distributed_accuracy' in df.columns and df['distributed_accuracy'].notna().any():\n",
    "            axes[1, 0].plot(df['round'], df['distributed_accuracy'], 'b-o', linewidth=2, markersize=6)\n",
    "            axes[1, 0].set_title('Distributed accuracy')\n",
    "            axes[1, 0].set_xlabel('Round')\n",
    "            axes[1, 0].set_ylabel('Accuracy')\n",
    "            axes[1, 0].grid(True, alpha=0.3)\n",
    "            axes[1, 0].set_ylim([0, 1])\n",
    "        \n",
    "        # 4. Training progress summary\n",
    "        if 'distributed_accuracy' in df.columns and df['distributed_accuracy'].notna().any():\n",
    "            improvement = df['distributed_accuracy'].iloc[-1] - df['distributed_accuracy'].iloc[0]\n",
    "            axes[1, 1].bar(['Initial', 'Final'],\n",
    "                           [df['distributed_accuracy'].iloc[0], df['distributed_accuracy'].iloc[-1]],\n",
    "                           color=['lightblue', 'darkblue'], alpha=0.7)\n",
    "            axes[1, 1].set_title(f\"Accuracy improvement: +{improvement: .3f}\")\n",
    "            axes[1, 1].set_ylabel('Accuracy')\n",
    "            axes[1, 1].set_ylim([0, 1])\n",
    "            axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Summary statistics\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(\"FEDERATED LEARNING SUMMARY\")\n",
    "        print(f\"{'='*50}\")\n",
    "        print(f\"Number of Clients: {NUM_CLIENTS}\")\n",
    "        print(f\"Number of Rounds: {NUM_ROUNDS}\")\n",
    "        print(f\"Device Used: {DEVICE}\")\n",
    "        \n",
    "        if 'distributed_accuracy' in df.columns and df['distributed_accuracy'].notna().any():\n",
    "            initial_acc = df['distributed_accuracy'].iloc[0]\n",
    "            final_acc = df['distributed_accuracy'].iloc[-1]\n",
    "            max_acc = df['distributed_accuracy'].max()\n",
    "            print(f\"\\nAccuracy Metrics:\")\n",
    "            print(f\"  Initial Accuracy: {initial_acc:.4f}\")\n",
    "            print(f\"  Final Accuracy: {final_acc:.4f}\")\n",
    "            print(f\"  Best Accuracy: {max_acc:.4f}\")\n",
    "            print(f\"  Total Improvement: {final_acc - initial_acc:.4f}\")\n",
    "        \n",
    "        if 'distributed_loss' in df.columns and df['distributed_loss'].notna().any():\n",
    "            initial_loss = df['distributed_loss'].iloc[0]\n",
    "            final_loss = df['distributed_loss'].iloc[-1]\n",
    "            min_loss = df['distributed_loss'].min()\n",
    "            print(f\"\\nLoss Metrics:\")\n",
    "            print(f\"  Initial Loss: {initial_loss:.4f}\")\n",
    "            print(f\"  Final Loss: {final_loss:.4f}\")\n",
    "            print(f\"  Best Loss: {min_loss:.4f}\")\n",
    "            print(f\"  Loss Reduction: {initial_loss - final_loss:.4f}\")\n",
    "        \n",
    "        return df\n",
    "    else:\n",
    "        print(\"No metrics data found in history object\")\n",
    "        print(\"Available history attributes:\")\n",
    "        for attr in dir(history):\n",
    "            if not attr.startswith('_'):\n",
    "                print(f\"  - {attr}: {getattr(history, attr)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "results_df = analyze_fl_results(fl_history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
